{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(os.getcwd()).parent))\n",
    "print(f\"Folder \\033[1m{sys.path[-1]}\\033[0m added to PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = '../sample_data/UC_Berkeley.pdf'\n",
    "QA_DATASET = '../sample_data/rag/'\n",
    "FINETUNED_MODEL = 'llama-2-7b-berkeley'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_dataset import main as generate_dataset_main\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_critic = ChatOpenAI(model=\"gpt-4-0125-preview\")\n",
    "llm_emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "generate_dataset_main(llm_critic, llm_emb, file_path=INPUT_FILE, output=QA_DATASET,\n",
    "                      chunk_size=512, num_questions=3, max_chunks=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Model Fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finetune_model import main as finetune_main, DEFAULT_MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_main(QA_DATASET, DEFAULT_MODEL_NAME, FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finetune_model import get_tokenizer_model, format_instruction\n",
    "from datasets import load_from_disk\n",
    "from transformers import pipeline\n",
    "\n",
    "model, tokenizer = get_tokenizer_model(FINETUNED_MODEL, inference_mode=True)\n",
    "dataset = load_from_disk(QA_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response\n",
    "test = dataset[1]\n",
    "del(test[\"prompt_cot_response\"])\n",
    "query = format_instruction(test)\n",
    "print(query)\n",
    "text_gen = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1024, temperature=0)\n",
    "output = text_gen(query)\n",
    "print(f\"\\033[92m{output[0]['generated_text']}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "mistral"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
